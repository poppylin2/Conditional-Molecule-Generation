{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1914692d",
   "metadata": {},
   "source": [
    "# ðŸ§¬ molGPT: Conditional Molecular Generation with Transformers\n",
    "\n",
    "**molGPT** is an end-to-end pipeline for generating novel drug-like molecules using a transformer-based language model (GPT-style). This project focuses on **conditional SMILES generation**, where molecular properties such as LogP, QED, TPSA, and scaffold are used to guide the generation process. It's an exciting intersection of **natural language processing** and **computational drug discovery**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Highlights\n",
    "\n",
    "- âœ… Trains a decoder-only transformer (GPT) to generate valid SMILES strings\n",
    "- ðŸŽ¯ Conditioned on molecular properties like:\n",
    "  - **LogP** (lipophilicity)\n",
    "  - **QED** (quantitative estimate of drug-likeness)\n",
    "  - **TPSA** (topological polar surface area)\n",
    "  - **Scaffold** (molecular backbone)\n",
    "- ðŸ“Š Includes evaluation metrics:\n",
    "  - SMILES validity\n",
    "  - Molecular uniqueness\n",
    "  - Structural novelty (Tanimoto similarity)\n",
    "  - Property alignment\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Dataset\n",
    "\n",
    "Uses the [MOSES](https://github.com/molecularsets/moses) dataset â€” a curated collection of drug-like molecules, suitable for generative modeling.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c63098b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T10:50:13.163600Z",
     "iopub.status.busy": "2025-04-23T10:50:13.163349Z",
     "iopub.status.idle": "2025-04-23T10:51:39.925237Z",
     "shell.execute_reply": "2025-04-23T10:51:39.924435Z",
     "shell.execute_reply.started": "2025-04-23T10:50:13.163581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: pathos in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /usr/local/lib/python3.11/dist-packages (from pathos) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from pathos) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /usr/local/lib/python3.11/dist-packages (from pathos) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /usr/local/lib/python3.11/dist-packages (from pathos) (0.70.16)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "!pip install pandas rdkit transformers[torch] accelerate>=0.26.0\n",
    "!pip install scikit-learn matplotlib tqdm pathos\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install git+https://github.com/molecularsets/moses.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dffc94",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab94ece2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T10:51:39.926842Z",
     "iopub.status.busy": "2025-04-23T10:51:39.926609Z",
     "iopub.status.idle": "2025-04-23T10:52:07.936477Z",
     "shell.execute_reply": "2025-04-23T10:52:07.935824Z",
     "shell.execute_reply.started": "2025-04-23T10:51:39.926821Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 10:51:53.734325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745405513.959311      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745405514.024374      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from functools import partial\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw, Descriptors, QED, rdMolDescriptors\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.Scaffolds.MurckoScaffold import GetScaffoldForMol\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec9d07",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Filtering\n",
    "\n",
    " We'll use the MOSES dataset, which is a curated set of drug-like molecules specifically\n",
    " designed for machine learning applications. It's much smaller than ChEMBL\n",
    " (https://www.ebi.ac.uk/chembl/, database: https://chembl.gitbook.io/chembl-interface-documentation/)\n",
    " but still contains high-quality, drug-like compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03667ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T10:52:07.937700Z",
     "iopub.status.busy": "2025-04-23T10:52:07.937214Z",
     "iopub.status.idle": "2025-04-23T10:52:10.169605Z",
     "shell.execute_reply": "2025-04-23T10:52:10.168759Z",
     "shell.execute_reply.started": "2025-04-23T10:52:07.937683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-filtered dataset from /kaggle/input/dataset-v1-filtered/dataset_v1_filtered.csv\n",
      "Loaded 1735494 pre-filtered drug-like molecules\n",
      "\n",
      "First few rows of filtered dataset:\n",
      "                                   SMILES  SPLIT\n",
      "0  CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1  train\n",
      "1    CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1  train\n",
      "2  CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1   test\n",
      "3     Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO  train\n",
      "4        Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C  train\n"
     ]
    }
   ],
   "source": [
    "def load_moses_data():\n",
    "    filtered_path = Path('/kaggle/input/dataset-v1-filtered/dataset_v1_filtered.csv')\n",
    "    if filtered_path.exists():\n",
    "      print(f\"Loading pre-filtered dataset from {filtered_path}\")\n",
    "      df = pd.read_csv(filtered_path)\n",
    "      print(f\"Loaded {len(df)} pre-filtered drug-like molecules\")\n",
    "      return df\n",
    "    train_path = Path('/kaggle/input/dataset-v1/dataset_v1.csv')\n",
    "    print(f\"Reading the original dataset from {train_path}\")\n",
    "    df = pd.read_csv(train_path)\n",
    "    print(df.info)\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "    if 'smiles' in df.columns:\n",
    "      df = df.rename(columns={'smiles': 'SMILES'})\n",
    "    elif 'SMILES' not in df.columns:\n",
    "      print(\"Available columns:\", df.columns.tolist())\n",
    "      raise ValueError(\"No 'smiles' or 'SMILES' column found in the dataset.\")\n",
    "    smiles_list = df['SMILES'].values\n",
    "    print(f\"\\nFound {len(smiles_list)} SMILES strings in the dataset.\")\n",
    "\n",
    "    # Here we begin with filtering\n",
    "    valid_mols = []\n",
    "    for smi in tqdm(smiles_list, desc=\"Validating and filtering SMILES\"):\n",
    "      # Filter 1 ensures that all molecules are chemically and structurally valid\n",
    "      mol = Chem.MolFromSmiles(smi)\n",
    "      if mol is not None:\n",
    "        # Filter 2 makes sure molecules with physicochemical properties out of a desirable range are removed from the list\n",
    "        mw = Descriptors.ExactMolWt(mol) # Molecular weight (in Da units)\n",
    "        logp = Descriptors.MolLogP(mol) # LogP(measured lipophilicity, i.e., how much a molecule likes to be solved in fat versus water)\n",
    "        hbd = rdMolDescriptors.CalcNumHBD(mol) # Number of a molecule's hydrogen-bond donor heavy atoms\n",
    "        hba = rdMolDescriptors.CalcNumHBA(mol) # Number of a molecule's hydrogen-bond acceptor heavy atoms\n",
    "\n",
    "        if mw <= 500 and logp <=5 and hbd <= 5 and hba <= 10:\n",
    "          # Filter 3 screens for problematic chemical groups shown to be associated with toxicity, carcinogenicity, etc.\n",
    "          has_bad_groups = False\n",
    "          patt_list = [\n",
    "              '[N+]([O-])=O',  # Nitro groups: Highly reactive, can cause DNA damage and carcinogenicity\n",
    "              '[S](=[O])(=[O])',  # Sulfonyl groups: Can be chemically reactive and cause skin/eye irritation\n",
    "              '[P](=[O])',  # Phosphoryl groups: Potential toxicity and instability in biological systems\n",
    "              '[As]'  # Arsenic: Highly toxic heavy metal with severe health risks and carcinogenic properties\n",
    "          ]\n",
    "          for patt in patt_list:\n",
    "              if mol.HasSubstructMatch(Chem.MolFromSmarts(patt)):\n",
    "                has_bad_groups = True\n",
    "                break\n",
    "          if not has_bad_groups:\n",
    "            valid_mols.append(smi)\n",
    "\n",
    "    # Create the filtered dataframe\n",
    "    filtered_df = df[df['SMILES'].isin(valid_mols)]\n",
    "    print(f\"\\nAfter filtering, {len(filtered_df)} molecules remain\")\n",
    "    return filtered_df\n",
    "\n",
    "# Load and display filtered data\n",
    "filtered_df = load_moses_data()\n",
    "print(\"\\nFirst few rows of filtered dataset:\")\n",
    "print(filtered_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9fba43-be7e-43f0-9d8b-1d55efa0063f",
   "metadata": {},
   "source": [
    "## 3. Descriptor Calculation (Scaffolds, logP, QED, TPSA)\n",
    " We compute additional descriptors needed:\n",
    " - Murcko Scaffolds: Core molecular framework obtained by removing side chains and keeping only ring systems and linkers between rings\n",
    " - QED\n",
    " - TPSA\n",
    " - LogP\n",
    "\n",
    "We'll store these in the DataFrame alongside the SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcbb672-8297-4ccc-9a96-7338c7354497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T10:52:10.171466Z",
     "iopub.status.busy": "2025-04-23T10:52:10.170627Z",
     "iopub.status.idle": "2025-04-23T11:13:47.724275Z",
     "shell.execute_reply": "2025-04-23T11:13:47.723478Z",
     "shell.execute_reply.started": "2025-04-23T10:52:10.171434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating molecular descriptors...\n",
      "Detected 4 CPU cores\n",
      "Running descriptor calculations in parallel across 4 cores\n",
      "Processing 1735494 SMILES strings in 17355 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acdad4c88a44341a7fb5a5797f17566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/17355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size after descriptor calculation: 1735494\n",
      "\n",
      "Descriptor Statistics:\n",
      "LogP range: -4.16 to 5.00\n",
      "QED range: 0.21 to 0.95\n",
      "TPSA range: 0.00 to 206.50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>SPLIT</th>\n",
       "      <th>Scaffold</th>\n",
       "      <th>LogP</th>\n",
       "      <th>QED</th>\n",
       "      <th>TPSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1</td>\n",
       "      <td>train</td>\n",
       "      <td>N=c1[nH]c2ccccc2[nH]1</td>\n",
       "      <td>1.68070</td>\n",
       "      <td>0.896898</td>\n",
       "      <td>87.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1</td>\n",
       "      <td>train</td>\n",
       "      <td>c1ccc(OCn2ccnc2)cc1</td>\n",
       "      <td>3.72930</td>\n",
       "      <td>0.862259</td>\n",
       "      <td>44.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1</td>\n",
       "      <td>test</td>\n",
       "      <td>O=C(NCC1CC2CCC1C2)c1ccccc1</td>\n",
       "      <td>3.45670</td>\n",
       "      <td>0.901948</td>\n",
       "      <td>40.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO</td>\n",
       "      <td>train</td>\n",
       "      <td>c1ccc(Nc2ccccn2)cc1</td>\n",
       "      <td>2.29702</td>\n",
       "      <td>0.701022</td>\n",
       "      <td>91.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C</td>\n",
       "      <td>train</td>\n",
       "      <td>O=c1[nH]c(=O)c2[nH]cnc2[nH]1</td>\n",
       "      <td>-2.21310</td>\n",
       "      <td>0.646083</td>\n",
       "      <td>102.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   SMILES  SPLIT  \\\n",
       "0  CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1  train   \n",
       "1    CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1  train   \n",
       "2  CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1   test   \n",
       "3     Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO  train   \n",
       "4        Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C  train   \n",
       "\n",
       "                       Scaffold     LogP       QED    TPSA  \n",
       "0         N=c1[nH]c2ccccc2[nH]1  1.68070  0.896898   87.31  \n",
       "1           c1ccc(OCn2ccnc2)cc1  3.72930  0.862259   44.12  \n",
       "2    O=C(NCC1CC2CCC1C2)c1ccccc1  3.45670  0.901948   40.54  \n",
       "3           c1ccc(Nc2ccccn2)cc1  2.29702  0.701022   91.68  \n",
       "4  O=c1[nH]c(=O)c2[nH]cnc2[nH]1 -2.21310  0.646083  102.28  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_descriptors(smiles: str):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        return None, None, None, None\n",
    "\n",
    "    try:\n",
    "        scaffold = GetScaffoldForMol(mol)\n",
    "        scaffold_smiles = Chem.MolToSmiles(scaffold)\n",
    "\n",
    "        qed_val = QED.qed(mol)\n",
    "\n",
    "        tpsa_val = rdMolDescriptors.CalcTPSA(mol)\n",
    "\n",
    "        logp_val = Descriptors.MolLogP(mol)\n",
    "\n",
    "        return scaffold_smiles, logp_val, qed_val, tpsa_val\n",
    "    except:\n",
    "        return None, None, None, None\n",
    "\n",
    "def process_batch(smiles_batch):\n",
    "    results = []\n",
    "    for smi in smiles_batch:\n",
    "        results.append(calculate_descriptors(smi))\n",
    "    return results\n",
    "\n",
    "def calculate_descriptors_parallel(df, parallel=True, batch_size=100):\n",
    "    print(\"Calculating molecular descriptors...\")\n",
    "\n",
    "    if parallel:\n",
    "        n_cores = Pool().ncpus\n",
    "        print(f\"Detected {n_cores} CPU cores\")\n",
    "        print(f\"Running descriptor calculations in parallel across {n_cores} cores\")\n",
    "\n",
    "        smiles_list = df['SMILES'].tolist()\n",
    "        n_batches = (len(smiles_list) + batch_size - 1) // batch_size\n",
    "        batches = [smiles_list[i*batch_size:(i+1)*batch_size]\n",
    "                  for i in range(n_batches)]\n",
    "\n",
    "        print(f\"Processing {len(smiles_list)} SMILES strings in {n_batches} batches\")\n",
    "\n",
    "        with Pool() as pool:\n",
    "            results = list(tqdm(\n",
    "                pool.imap(process_batch, batches),\n",
    "                total=n_batches,\n",
    "                desc=\"Processing batches\"\n",
    "            ))\n",
    "\n",
    "        all_results = [item for batch in results for item in batch]\n",
    "\n",
    "    else:\n",
    "        print(\"Running descriptor calculations sequentially\")\n",
    "        all_results = []\n",
    "        for smi in tqdm(df['SMILES'], desc=\"Calculating descriptors\"):\n",
    "            all_results.append(calculate_descriptors(smi))\n",
    "\n",
    "    scaffolds, logps, qeds, tpsas = zip(*all_results)\n",
    "\n",
    "    df['Scaffold'] = scaffolds\n",
    "    df['LogP'] = logps\n",
    "    df['QED'] = qeds\n",
    "    df['TPSA'] = tpsas\n",
    "\n",
    "    df = df.dropna(subset=['Scaffold', 'LogP', 'QED', 'TPSA'])\n",
    "    print(f\"Final dataset size after descriptor calculation: {len(df)}\")\n",
    "\n",
    "    print(\"\\nDescriptor Statistics:\")\n",
    "    print(f\"LogP range: {df['LogP'].min():.2f} to {df['LogP'].max():.2f}\")\n",
    "    print(f\"QED range: {df['QED'].min():.2f} to {df['QED'].max():.2f}\")\n",
    "    print(f\"TPSA range: {df['TPSA'].min():.2f} to {df['TPSA'].max():.2f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "filtered_df = calculate_descriptors_parallel(filtered_df, parallel=True)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9aad1-2dfe-4913-aaee-4c191e3338a9",
   "metadata": {},
   "source": [
    "## 4. Preparing the Data for Conditional Generation\n",
    "\n",
    "We will train a GPT model to generate the full SMILES given:\n",
    "1. The scaffold SMILES\n",
    "2. The desired LogP\n",
    "3. The desired QED\n",
    "4. The desired TPSA\n",
    "\n",
    "One straightforward approach is to serialize these conditions into a single text string.\n",
    "For example:\n",
    "\n",
    "    \"SCAFFOLD: Cc1ccccn1 | LOGP: 2.3 | QED: 0.72 | TPSA: 32.4 => FULL_SMILES\"\n",
    "\n",
    "We then train the model in a language modeling fashion to predict FULL_SMILES from\n",
    "these inputs. Alternatively, we could build a more sophisticated approach that\n",
    "incorporates the numeric data differently, but for demonstration, we'll do it text-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4f5076-a7a8-4292-a64a-6fccf1e617ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:13:47.726664Z",
     "iopub.status.busy": "2025-04-23T11:13:47.726395Z",
     "iopub.status.idle": "2025-04-23T11:14:08.219192Z",
     "shell.execute_reply": "2025-04-23T11:14:08.218552Z",
     "shell.execute_reply.started": "2025-04-23T11:13:47.726646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>SPLIT</th>\n",
       "      <th>Scaffold</th>\n",
       "      <th>LogP</th>\n",
       "      <th>QED</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>conditional_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1</td>\n",
       "      <td>train</td>\n",
       "      <td>N=c1[nH]c2ccccc2[nH]1</td>\n",
       "      <td>1.68070</td>\n",
       "      <td>0.896898</td>\n",
       "      <td>87.31</td>\n",
       "      <td>SCAFFOLD: N=c1[nH]c2ccccc2[nH]1 | LOGP: 1.68 |...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1</td>\n",
       "      <td>train</td>\n",
       "      <td>c1ccc(OCn2ccnc2)cc1</td>\n",
       "      <td>3.72930</td>\n",
       "      <td>0.862259</td>\n",
       "      <td>44.12</td>\n",
       "      <td>SCAFFOLD: c1ccc(OCn2ccnc2)cc1 | LOGP: 3.73 | Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1</td>\n",
       "      <td>test</td>\n",
       "      <td>O=C(NCC1CC2CCC1C2)c1ccccc1</td>\n",
       "      <td>3.45670</td>\n",
       "      <td>0.901948</td>\n",
       "      <td>40.54</td>\n",
       "      <td>SCAFFOLD: O=C(NCC1CC2CCC1C2)c1ccccc1 | LOGP: 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO</td>\n",
       "      <td>train</td>\n",
       "      <td>c1ccc(Nc2ccccn2)cc1</td>\n",
       "      <td>2.29702</td>\n",
       "      <td>0.701022</td>\n",
       "      <td>91.68</td>\n",
       "      <td>SCAFFOLD: c1ccc(Nc2ccccn2)cc1 | LOGP: 2.30 | Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C</td>\n",
       "      <td>train</td>\n",
       "      <td>O=c1[nH]c(=O)c2[nH]cnc2[nH]1</td>\n",
       "      <td>-2.21310</td>\n",
       "      <td>0.646083</td>\n",
       "      <td>102.28</td>\n",
       "      <td>SCAFFOLD: O=c1[nH]c(=O)c2[nH]cnc2[nH]1 | LOGP:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   SMILES  SPLIT  \\\n",
       "0  CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1  train   \n",
       "1    CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1  train   \n",
       "2  CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1   test   \n",
       "3     Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO  train   \n",
       "4        Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C  train   \n",
       "\n",
       "                       Scaffold     LogP       QED    TPSA  \\\n",
       "0         N=c1[nH]c2ccccc2[nH]1  1.68070  0.896898   87.31   \n",
       "1           c1ccc(OCn2ccnc2)cc1  3.72930  0.862259   44.12   \n",
       "2    O=C(NCC1CC2CCC1C2)c1ccccc1  3.45670  0.901948   40.54   \n",
       "3           c1ccc(Nc2ccccn2)cc1  2.29702  0.701022   91.68   \n",
       "4  O=c1[nH]c(=O)c2[nH]cnc2[nH]1 -2.21310  0.646083  102.28   \n",
       "\n",
       "                                    conditional_text  \n",
       "0  SCAFFOLD: N=c1[nH]c2ccccc2[nH]1 | LOGP: 1.68 |...  \n",
       "1  SCAFFOLD: c1ccc(OCn2ccnc2)cc1 | LOGP: 3.73 | Q...  \n",
       "2  SCAFFOLD: O=C(NCC1CC2CCC1C2)c1ccccc1 | LOGP: 3...  \n",
       "3  SCAFFOLD: c1ccc(Nc2ccccn2)cc1 | LOGP: 2.30 | Q...  \n",
       "4  SCAFFOLD: O=c1[nH]c(=O)c2[nH]cnc2[nH]1 | LOGP:...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_condition_text(row):\n",
    "  scaffold_str = row['Scaffold']\n",
    "  logp_str = f\"{row['LogP']:.2f}\"\n",
    "  qed_str = f\"{row['QED']:.2f}\"\n",
    "  tpsa_str = f\"{row['TPSA']:.1f}\"\n",
    "  full_smiles = row['SMILES']\n",
    "\n",
    "  input_text = f\"SCAFFOLD: {scaffold_str} | LOGP: {logp_str} | QED: {qed_str} | TPSA: {tpsa_str} => f{full_smiles}\"\n",
    "  return input_text\n",
    "\n",
    "filtered_df['conditional_text'] = filtered_df.apply(create_condition_text, axis=1)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6186c-1784-4e82-87bf-bb9230e24133",
   "metadata": {},
   "source": [
    "## 5. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c54a27-39ee-4e40-89e7-fc5b2af02326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:14:08.220036Z",
     "iopub.status.busy": "2025-04-23T11:14:08.219847Z",
     "iopub.status.idle": "2025-04-23T11:14:09.555653Z",
     "shell.execute_reply": "2025-04-23T11:14:09.554852Z",
     "shell.execute_reply.started": "2025-04-23T11:14:08.220019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1388395\n",
      "Val size: 173549\n",
      "Test size: 173550\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valtest_df = train_test_split(filtered_df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(valtest_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Val size: {len(val_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635667d4-da28-49fd-a6b0-04d2ea374f35",
   "metadata": {},
   "source": [
    "## 6. Defining the GPT Model and Tokenizer\n",
    "We'll create a small GPT2 model from scratch (or you could fine-tune a pretrained GPT2).\n",
    "For demonstration, we'll use a smaller config for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f91f9a-b2f0-463f-894f-93be492b4530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:14:09.556742Z",
     "iopub.status.busy": "2025-04-23T11:14:09.556479Z",
     "iopub.status.idle": "2025-04-23T11:14:12.343611Z",
     "shell.execute_reply": "2025-04-23T11:14:12.342808Z",
     "shell.execute_reply.started": "2025-04-23T11:14:09.556714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08f26554fec4b6a92089d33c3e28243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d34a28a1774db784632e8ef93c9d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fd05d02ec64e02afb15904de36e018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1a6ac9ed764480a44b166c24e51c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca6565843b74ea987768fb30e6c7d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=256,\n",
    "    n_ctx=256,\n",
    "    n_embd=128,\n",
    "    n_layer=4,\n",
    "    n_head=4\n",
    ")\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6102df3d-31ad-4a65-b671-ac97bbb07584",
   "metadata": {},
   "source": [
    "## 7. Create a Custom Dataset for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11720886-a110-49b8-bce0-1ba8cc70cd0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:14:12.344746Z",
     "iopub.status.busy": "2025-04-23T11:14:12.344480Z",
     "iopub.status.idle": "2025-04-23T11:14:12.350605Z",
     "shell.execute_reply": "2025-04-23T11:14:12.349772Z",
     "shell.execute_reply.started": "2025-04-23T11:14:12.344715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SmilesConditionalDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.data.iloc[idx]['conditional_text']\n",
    "\n",
    "        return self.tokenizer(\n",
    "            txt,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_special_tokens_mask=True\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = SmilesConditionalDataset(train_df, tokenizer)\n",
    "val_dataset   = SmilesConditionalDataset(val_df, tokenizer)\n",
    "test_dataset  = SmilesConditionalDataset(test_df, tokenizer)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5eaee2-3c60-4528-be15-c243447806fa",
   "metadata": {},
   "source": [
    "## 8. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bea16abd-6fc9-4878-9715-f77524812c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:14:12.351529Z",
     "iopub.status.busy": "2025-04-23T11:14:12.351349Z",
     "iopub.status.idle": "2025-04-23T14:33:47.096986Z",
     "shell.execute_reply": "2025-04-23T14:33:47.096230Z",
     "shell.execute_reply.started": "2025-04-23T11:14:12.351515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32538' max='32538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32538/32538 3:19:31, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.292600</td>\n",
       "      <td>0.555617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>0.493483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>0.464609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.447951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.436301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.467700</td>\n",
       "      <td>0.425407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.419265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.447200</td>\n",
       "      <td>0.413480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0.407734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.437800</td>\n",
       "      <td>0.403773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.399263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.396149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.392759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.390271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.387397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.385891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.383352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.409700</td>\n",
       "      <td>0.381025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.379171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.404800</td>\n",
       "      <td>0.377293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.403600</td>\n",
       "      <td>0.375811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32538, training_loss=0.47483191506879785, metrics={'train_runtime': 11973.771, 'train_samples_per_second': 347.859, 'train_steps_per_second': 2.717, 'total_flos': 1905046386204672.0, 'train_loss': 0.47483191506879785, 'epoch': 2.999919331604725})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./conditional_gpt_smiles\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=50,\n",
    "#     save_steps=50,\n",
    "#     logging_steps=50,\n",
    "#     save_total_limit=1,\n",
    "#     learning_rate=1e-4,\n",
    "#     warmup_steps=100,\n",
    "#     weight_decay=0.01,\n",
    "#     run_name=\"gpt-smiles-cond-epoch1\",                # <--- é¿å… W&B çš„ warning\n",
    "#     report_to=\"none\",               # <--- å¯ç”¨ wandb é›†æˆï¼ˆè‡ªåŠ¨è®°å½• metricsï¼‰\n",
    " \n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./conditional_gpt_smiles\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,        # Increased\n",
    "    per_device_eval_batch_size=16,         # Increased\n",
    "    gradient_accumulation_steps=8,        # Added\n",
    "    fp16=True,                            # Added for mixed precision\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1500,                       # Reduced frequency\n",
    "    save_steps=1500,                       # Reduced frequency\n",
    "    logging_steps=1200,                    # Reduced frequency\n",
    "    save_total_limit=1,\n",
    "    learning_rate=1e-3,                   # Adjusted for larger batch\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_num_workers=4,             # Added\n",
    "    run_name=\"gpt-smiles-cond-epoch1\",               \n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./conditional_gpt_smiles\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     fp16=True,\n",
    "#     fp16_opt_level=\"O2\",           # More aggressive mixed precision\n",
    "#     eval_strategy=\"steps\",\n",
    "#     eval_steps=500,\n",
    "#     save_steps=500,\n",
    "#     logging_steps=200,\n",
    "#     save_total_limit=1,\n",
    "#     learning_rate=1e-3,            # Adjusted for larger effective batch\n",
    "#     warmup_steps=100,\n",
    "#     weight_decay=0.01,\n",
    "#     dataloader_num_workers=8,\n",
    "#     dataloader_pin_memory=True,    # Faster data transfer\n",
    "#     gradient_checkpointing=True,   # Memory efficient training\n",
    "#     run_name=\"gpt-smiles-cond-epoch1\",\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374c273-e0fd-4dbf-afb3-ade2b780dc41",
   "metadata": {},
   "source": [
    "## 9. Inference (Generating SMILES from Conditions)\n",
    "\n",
    "During inference, we'll provide the scaffold and desired property values. For example:\n",
    "\n",
    "\"Scaffold: <scaffold> | LogP: <val> | QED: <val> | TPSA: <val> =>\"\n",
    "\n",
    "The model should complete the sequence by generating a SMILES string.\n",
    "\n",
    "We'll generate multiple samples to test uniqueness and validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e6aed5-b0d5-496b-a22e-386bb724160b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:53:37.816177Z",
     "iopub.status.busy": "2025-04-23T14:53:37.815523Z",
     "iopub.status.idle": "2025-04-23T14:53:37.825776Z",
     "shell.execute_reply": "2025-04-23T14:53:37.825056Z",
     "shell.execute_reply.started": "2025-04-23T14:53:37.816122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡è¯„ä¼°æ¨¡åž‹...\n",
      "æ¨¡åž‹å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def generate_smiles_from_conditions(model, tokenizer, scaffold, logp, qed, tpsa,\n",
    "#                                     max_length=256, num_return_sequences=1):\n",
    "#     prompt = f\"Scaffold: {scaffold} | LogP: {logp:.2f} | QED: {qed:.2f} | TPSA: {tpsa:.2f} =>\"\n",
    "\n",
    "#     input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "#     input_ids = input_ids.to(model.device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             input_ids=input_ids,\n",
    "#             max_length=max_length,\n",
    "#             num_return_sequences=num_return_sequences,\n",
    "#             do_sample=True,           # Use sampling\n",
    "#             top_k=50,                 # Adjust as desired\n",
    "#             top_p=0.95,               # Adjust as desired\n",
    "#             temperature=0.7,          # Adjust as desired\n",
    "#             pad_token_id=tokenizer.eos_token_id\n",
    "#         )\n",
    "\n",
    "#     generated_texts = []\n",
    "#     for output in outputs:\n",
    "#         text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "#         # We want only the SMILES part after the \"=>\"\n",
    "#         if \"=>\" in text:\n",
    "#             smiles_part = text.split(\"=>\")[-1].strip()\n",
    "#             generated_texts.append(smiles_part)\n",
    "#         else:\n",
    "#             generated_texts.append(text)\n",
    "\n",
    "#     return generated_texts\n",
    "\n",
    "# trainer.model.eval()\n",
    "# row = test_df.iloc[0]\n",
    "# generated_smiles = generate_smiles_from_conditions(\n",
    "#     model=trainer.model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     scaffold=row['Scaffold'],\n",
    "#     logp=row['LogP'],\n",
    "#     qed=row['QED'],\n",
    "#     tpsa=row['TPSA'],\n",
    "#     num_return_sequences=5\n",
    "# )\n",
    "# # print(generated_smiles)\n",
    "\n",
    "\n",
    "def generate_smiles_from_conditions(model, tokenizer, scaffold, logp, qed, tpsa,\n",
    "                                    max_length=256, num_return_sequences=1):\n",
    "    prompt = f\"SCAFFOLD: {scaffold} | LOGP: {logp:.2f} | QED: {qed:.2f} | TPSA: {tpsa:.2f} =>\"\n",
    "\n",
    "    # æ˜¾å¼åˆ›å»ºæ³¨æ„åŠ›æŽ©ç \n",
    "    encoded_input = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "    input_ids = encoded_input['input_ids'].to(model.device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(model.device)\n",
    "    \n",
    "    print(f\"ç”Ÿæˆæ¡ä»¶: {prompt}\")\n",
    "    print(f\"è¾“å…¥åºåˆ—é•¿åº¦: {input_ids.shape}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,           # Use sampling\n",
    "            top_k=100,                # Increased from 50 to 100\n",
    "            top_p=0.9,                # Lowered from 0.95 to 0.9\n",
    "            temperature=0.5,          # Lowered from 0.7 to 0.5\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        #     print(\"æ¨¡åž‹ç”Ÿæˆå®Œæˆ\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        #     return []\n",
    "\n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(f\"åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: {text}\")\n",
    "        # æˆ‘ä»¬åªéœ€è¦ \"=>\" åŽé¢çš„SMILESéƒ¨åˆ†\n",
    "        if \"=>\" in text:\n",
    "            smiles_part = text.split(\"=>\")[-1].strip()\n",
    "            generated_texts.append(smiles_part)\n",
    "        else:\n",
    "            generated_texts.append(text)\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "# æ·»åŠ è°ƒè¯•ä¿¡æ¯\n",
    "print(\"å‡†å¤‡è¯„ä¼°æ¨¡åž‹...\")\n",
    "trainer.model.eval()\n",
    "print(\"æ¨¡åž‹å·²è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\")\n",
    "\n",
    "# try:\n",
    "#     row = test_df.iloc[0]\n",
    "#     print(f\"æµ‹è¯•æ ·æœ¬: Scaffold={row['Scaffold']}, LogP={row['LogP']:.2f}, QED={row['QED']:.2f}, TPSA={row['TPSA']:.2f}\")\n",
    "    \n",
    "#     generated_smiles = generate_smiles_from_conditions(\n",
    "#         model=trainer.model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         scaffold=row['Scaffold'],\n",
    "#         logp=row['LogP'],\n",
    "#         qed=row['QED'],\n",
    "#         tpsa=row['TPSA'],\n",
    "#         num_return_sequences=1  # å…ˆå°è¯•åªç”Ÿæˆä¸€ä¸ªæ ·æœ¬\n",
    "#     )\n",
    "    \n",
    "#     print(\"ç”Ÿæˆç»“æžœ:\")\n",
    "#     for i, smi in enumerate(generated_smiles):\n",
    "#         print(f\"ç”Ÿæˆçš„SMILES {i+1}: {smi}\")\n",
    "#         mol = Chem.MolFromSmiles(smi)\n",
    "#         if mol:\n",
    "#             print(f\"æœ‰æ•ˆçš„SMILES: æ˜¯\")\n",
    "#         else:\n",
    "#             print(f\"æœ‰æ•ˆçš„SMILES: å¦\")\n",
    "# except Exception as e:\n",
    "#     print(f\"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8023d6b-6490-4742-973f-8967519f98a8",
   "metadata": {},
   "source": [
    "## 10. Evaluation\n",
    "We will:\n",
    "1. Generate 1000 molecules with random conditions from the test set.\n",
    "2. Check:\n",
    "   - Valid SMILES: Can RDKit parse them?\n",
    "   - Unique SMILES: How many are duplicates?\n",
    "   - Tanimoto similarity to training set.\n",
    "   - Distribution of predicted properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fada0260-88f8-4455-9191-7f84f609413c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:53:51.674320Z",
     "iopub.status.busy": "2025-04-23T14:53:51.673927Z",
     "iopub.status.idle": "2025-04-23T14:53:51.686490Z",
     "shell.execute_reply": "2025-04-23T14:53:51.685840Z",
     "shell.execute_reply.started": "2025-04-23T14:53:51.674296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_valid_smiles(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    return mol is not None\n",
    "\n",
    "def compute_tanimoto_similarity(smi1, smi2, radius=2, nBits=2048):\n",
    "    mol1 = Chem.MolFromSmiles(smi1)\n",
    "    mol2 = Chem.MolFromSmiles(smi2)\n",
    "    if mol1 is None or mol2 is None:\n",
    "        return None\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, radius, nBits=nBits)\n",
    "    fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, radius, nBits=nBits)\n",
    "    return rdMolDescriptors.TanimotoSimilarity(fp1, fp2)\n",
    "\n",
    "# Let's define a quick function to evaluate.\n",
    "# In practice, you might do more robust analysis.\n",
    "def evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    reference_df,\n",
    "    n_samples=1000\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    valid_count = 0\n",
    "    unique_smiles = set()\n",
    "    similarities = []\n",
    "\n",
    "    # We'll store the property differences if we want to check property distribution.\n",
    "    requested_logps, generated_logps = [], []\n",
    "    requested_qeds, generated_qeds = [], []\n",
    "    requested_tpsas, generated_tpsas = [], []\n",
    "\n",
    "    # Convert train_df SMILES to a list for similarity reference\n",
    "    train_smiles_list = train_df['SMILES'].tolist()\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Randomly select a row from the reference set or sample property values\n",
    "        row = reference_df.sample(n=1).iloc[0]\n",
    "        scaffold = row['Scaffold']\n",
    "        logp_req = row['LogP']\n",
    "        qed_req = row['QED']\n",
    "        tpsa_req = row['TPSA']\n",
    "\n",
    "        gen_smiles_list = generate_smiles_from_conditions(\n",
    "            model, tokenizer, scaffold, logp_req, qed_req, tpsa_req,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        gen_smi = gen_smiles_list[0]\n",
    "\n",
    "        if is_valid_smiles(gen_smi):\n",
    "            valid_count += 1\n",
    "            unique_smiles.add(Chem.MolToSmiles(Chem.MolFromSmiles(gen_smi)))  # canonical\n",
    "\n",
    "            # Tanimoto similarity (just to the original training data)\n",
    "            # We'll compute the max similarity to any molecule in the training set\n",
    "            # as a measure of novelty.\n",
    "            best_sim = 0\n",
    "            for train_smi in train_smiles_list[:1000]:  # limit to 1000 for speed\n",
    "                sim = compute_tanimoto_similarity(gen_smi, train_smi)\n",
    "                if sim is not None and sim > best_sim:\n",
    "                    best_sim = sim\n",
    "            similarities.append(best_sim)\n",
    "\n",
    "            # Check the property distribution if desired\n",
    "            gen_mol = Chem.MolFromSmiles(gen_smi)\n",
    "            if gen_mol:\n",
    "                gen_logp = Descriptors.MolLogP(gen_mol)\n",
    "                gen_qed = QED.qed(gen_mol)\n",
    "                gen_tpsa = Descriptors.TPSA(gen_mol)\n",
    "\n",
    "                requested_logps.append(logp_req)\n",
    "                generated_logps.append(gen_logp)\n",
    "                requested_qeds.append(qed_req)\n",
    "                generated_qeds.append(gen_qed)\n",
    "                requested_tpsas.append(tpsa_req)\n",
    "                generated_tpsas.append(gen_tpsa)\n",
    "\n",
    "    validity_ratio = valid_count / n_samples\n",
    "    uniqueness_ratio = len(unique_smiles) / n_samples\n",
    "    avg_similarity = np.mean(similarities) if similarities else 0\n",
    "\n",
    "    print(f\"Validity: {validity_ratio:.2f}\")\n",
    "    print(f\"Uniqueness: {uniqueness_ratio:.2f}\")\n",
    "    print(f\"Average Tanimoto similarity to training set: {avg_similarity:.2f}\")\n",
    "\n",
    "    # Plot property distribution comparisons\n",
    "    # For example, requested vs generated LogP\n",
    "    if len(requested_logps) > 0:\n",
    "        plt.figure()\n",
    "        plt.scatter(requested_logps, generated_logps, alpha=0.5)\n",
    "        plt.xlabel(\"Requested LogP\")\n",
    "        plt.ylabel(\"Generated LogP\")\n",
    "        plt.title(\"Requested vs. Generated LogP\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(requested_qeds, generated_qeds, alpha=0.5)\n",
    "        plt.xlabel(\"Requested QED\")\n",
    "        plt.ylabel(\"Generated QED\")\n",
    "        plt.title(\"Requested vs. Generated QED\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(requested_tpsas, generated_tpsas, alpha=0.5)\n",
    "        plt.xlabel(\"Requested TPSA\")\n",
    "        plt.ylabel(\"Generated TPSA\")\n",
    "        plt.title(\"Requested vs. Generated TPSA\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de55773d-8508-493a-a922-8bc4d724a86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:53:59.094311Z",
     "iopub.status.busy": "2025-04-23T14:53:59.093930Z",
     "iopub.status.idle": "2025-04-23T14:54:29.244004Z",
     "shell.execute_reply": "2025-04-23T14:54:29.243242Z",
     "shell.execute_reply.started": "2025-04-23T14:53:59.094286Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model evaluation with 1000 samples...\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: c1ccccc1 | LOGP: 2.35 | QED: 0.81 | TPSA: 38.77 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 34])\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccccc1 | LOGP: 2.35 | QED: 0.81 | TPSA: 38.77 => fCOC(=O)C(C)Nc1cccc(C(F)(F)F)c1F)C(C)C11C=O | TPSA: 81.2)ccc1F)O)N1CCO)C1)C111C1 | LOGP: TPSA:1)cccccc1)F)N)O)C#N)C1C1F)N)C1F)C1C#N1C#N1F)C#N1F)C#N1F)C#N1C#N1C#N1C#N1)C#N1F)C#N1F)C#N1F)C#N1C#N1F)C#N1F)C#N1F)C#N1F)C#N1F)C#N1F)C#N1F)C#N1FF#N1F)\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(NC1CCCCC1)c1cccnn1 | LOGP: 2.10 | QED: 0.91 | TPSA: 72.11 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 44])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:01] SMILES Parse Error: syntax error while parsing: fCOC(=O)C(C)Nc1cccc(C(F)(F)F)c1F)C(C)C11C=O\n",
      "[14:54:01] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:01] fCOC(=O)C(C)Nc1cccc(C(F)(F)F)c1F)C(C)C11C\n",
      "[14:54:01] ^\n",
      "[14:54:01] SMILES Parse Error: Failed parsing SMILES 'fCOC(=O)C(C)Nc1cccc(C(F)(F)F)c1F)C(C)C11C=O' for input: 'fCOC(=O)C(C)Nc1cccc(C(F)(F)F)c1F)C(C)C11C=O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(NC1CCCCC1)c1cccnn1 | LOGP: 2.10 | QED: 0.91 | TPSA: 72.11 => fCC(=O)Nc1ccc(C(=O)NC2CCCCC2)nn1C(C)C)C1CC1F1 | TPSA:2)F11111111111111111111111111 | LOGP: 0.4 | T1)1)nn1)C11)C11)C11114444444444444421 | TPS3142 | TPS3142 | TPS42 | T42 | T4CO4CO4CO42CO42C3CO4CO4CO4CO4C2CO4CO4CO4CO4CO4CO4CO4CO4C2C2C2CO2C2CO4C1C2C2C2CO2CO1CO1CO4C2C2CO2C\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(Cc1cnoc1)N1CCC(Oc2ccccc2)CC1 | LOGP: 3.52 | QED: 0.85 | TPSA: 55.57 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:02] SMILES Parse Error: syntax error while parsing: fCC(=O)Nc1ccc(C(=O)NC2CCCCC2)nn1C(C)C)C1CC1F1\n",
      "[14:54:02] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:02] fCC(=O)Nc1ccc(C(=O)NC2CCCCC2)nn1C(C)C)C1C\n",
      "[14:54:02] ^\n",
      "[14:54:02] SMILES Parse Error: Failed parsing SMILES 'fCC(=O)Nc1ccc(C(=O)NC2CCCCC2)nn1C(C)C)C1CC1F1' for input: 'fCC(=O)Nc1ccc(C(=O)NC2CCCCC2)nn1C(C)C)C1CC1F1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Cc1cnoc1)N1CCC(Oc2ccccc2)CC1 | LOGP: 3.52 | QED: 0.85 | TPSA: 55.57 => fCc1noc(C)c1CC(=O)N1CCC(Oc2ccc(Cl)cc2)CC1C1C)C1111111111111111111111111FO2 | T1)C1C2 | TPS21F4O4CO2 | TPS21F2C2 | TPS31F4CO4CO4CO4CO4CO2F4CO4CO4C2F4CO4C2CO4CO4CO2C2C2CO2C2C2C2)C2)C2C2C2CO2C2C2CO2C2CO4CO2C2CO2)CO4CO2C2C2C2C2CO2C2C2CO2CO2F31CO2CO2)CO2C2F\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(Nc1ccccc1)NC1(c2ncon2)CCCC1 | LOGP: 3.37 | QED: 0.86 | TPSA: 89.28 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:03] SMILES Parse Error: syntax error while parsing: fCc1noc(C)c1CC(=O)N1CCC(Oc2ccc(Cl)cc2)CC1C1C)C1111111111111111111111111FO2\n",
      "[14:54:03] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:03] fCc1noc(C)c1CC(=O)N1CCC(Oc2ccc(Cl)cc2)CC1\n",
      "[14:54:03] ^\n",
      "[14:54:03] SMILES Parse Error: Failed parsing SMILES 'fCc1noc(C)c1CC(=O)N1CCC(Oc2ccc(Cl)cc2)CC1C1C)C1111111111111111111111111FO2' for input: 'fCc1noc(C)c1CC(=O)N1CCC(Oc2ccc(Cl)cc2)CC1C1C)C1111111111111111111111111FO2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Nc1ccccc1)NC1(c2ncon2)CCCC1 | LOGP: 3.37 | QED: 0.86 | TPSA: 89.28 => fCOC(=O)c1cccc(NC(=O)NC2(c3noc(C)n3)CCCC2)c1C)C1C)C)C1C)C1C)C)C1C1C1C1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1 | LOGPPPPPPP: | LOGPP: 0.72 | TPSA: 0.82 | TPSA:)C1C1C1C1C1C1C1C1C1C1C1C1\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=c1[nH]c(Cc2cccc3cccnc23)nc2ccsc12 | LOGP: 3.12 | QED: 0.62 | TPSA: 58.64 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 54])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:04] SMILES Parse Error: syntax error while parsing: fCOC(=O)c1cccc(NC(=O)NC2(c3noc(C)n3)CCCC2)c1C)C1C)C)C1C)C1C)C)C1C1C1C1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1\n",
      "[14:54:04] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:04] fCOC(=O)c1cccc(NC(=O)NC2(c3noc(C)n3)CCCC2\n",
      "[14:54:04] ^\n",
      "[14:54:04] SMILES Parse Error: Failed parsing SMILES 'fCOC(=O)c1cccc(NC(=O)NC2(c3noc(C)n3)CCCC2)c1C)C1C)C)C1C)C1C)C)C1C1C1C1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1' for input: 'fCOC(=O)c1cccc(NC(=O)NC2(c3noc(C)n3)CCCC2)c1C)C1C)C)C1C)C1C)C)C1C1C1C1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=c1[nH]c(Cc2cccc3cccnc23)nc2ccsc12 | LOGP: 3.12 | QED: 0.62 | TPSA: 58.64 => fCc1cc2c(=O)[nH]c(Cc3cccc4cccnc34)nc2s1 | TPSA: 80.7 => fO)cc2n1Cc1ccsc1)O2)C1C1C1C1F)N1F)N1F)C1F)N1F)C1F)C1F)C1F)C1F)N1F)C1F)C1F)C1C1C1F)C1C1CC1CC1CC111F1C1F)C1C1CC1F1F)C1F1F1C1F11C1F1C1F1F1C11C1C1 | TPSA:F1F1 | TPSA:F)C1C1F)C1C1F\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(Cn1ccc2ccccc21)N=c1[nH]nc2ccccn12 | LOGP: 2.71 | QED: 0.61 | TPSA: 67.45 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 57])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:05] SMILES Parse Error: syntax error while parsing: fO)cc2n1Cc1ccsc1)O2)C1C1C1C1F)N1F)N1F)C1F)N1F)C1F)C1F)C1F)C1F)N1F)C1F)C1F)C1C1C1F)C1C1CC1CC1CC111F1C1F)C1C1CC1F1F)C1F1F1C1F11C1F1C1F1F1C11C1C1\n",
      "[14:54:05] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:05] fO)cc2n1Cc1ccsc1)O2)C1C1C1C1F)N1F)N1F)C1F\n",
      "[14:54:05] ^\n",
      "[14:54:05] SMILES Parse Error: Failed parsing SMILES 'fO)cc2n1Cc1ccsc1)O2)C1C1C1C1F)N1F)N1F)C1F)N1F)C1F)C1F)C1F)C1F)N1F)C1F)C1F)C1C1C1F)C1C1CC1CC1CC111F1C1F)C1C1CC1F1F)C1F1F1C1F11C1F1C1F1F1C11C1C1' for input: 'fO)cc2n1Cc1ccsc1)O2)C1C1C1C1F)N1F)N1F)C1F)N1F)C1F)C1F)C1F)C1F)N1F)C1F)C1F)C1C1C1F)C1C1CC1CC1CC111F1C1F)C1C1CC1F1F)C1F1F1C1F11C1F1C1F1F1C11C1C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Cn1ccc2ccccc21)N=c1[nH]nc2ccccn12 | LOGP: 2.71 | QED: 0.61 | TPSA: 67.45 => fO=C(Cn1ccc2ccccc21)N=c1[nH]nc2cc(Cl)ccn12 | TPSA:)n1C(F)F)c1ccccn1F)F)F)F)F)FF)F | TPSA:FFF | TPSA:F)c1F | TPSA:F)F)cn1FFF)cn1FFFFF)FFF)cn1FFFFFFFFFFFFFF | TPSA:F1cccccccccccccccccc1F)F)F | TPSA:F)F | TPSA:F | TPSA:n1F)cn1F)cccc1FFF)n1FF)cn1FFFFFF)n\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(c1occc1-c1ccccc1)N1CCNCC1 | LOGP: 1.88 | QED: 0.86 | TPSA: 62.99 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 51])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:06] SMILES Parse Error: syntax error while parsing: fO=C(Cn1ccc2ccccc21)N=c1[nH]nc2cc(Cl)ccn12\n",
      "[14:54:06] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:06] fO=C(Cn1ccc2ccccc21)N=c1[nH]nc2cc(Cl)ccn1\n",
      "[14:54:06] ^\n",
      "[14:54:06] SMILES Parse Error: Failed parsing SMILES 'fO=C(Cn1ccc2ccccc21)N=c1[nH]nc2cc(Cl)ccn12' for input: 'fO=C(Cn1ccc2ccccc21)N=c1[nH]nc2cc(Cl)ccn12'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(c1occc1-c1ccccc1)N1CCNCC1 | LOGP: 1.88 | QED: 0.86 | TPSA: 62.99 => fCC(=O)N1CCN(C(=O)c2occc2-c2ccccc2)CC1C(=O)OC)C1C)C1C | TPSA:)C11C1C)C1C11 | TPSA:: 581C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1 | TPSA:11C11C1C11C1)C11C1C11C1C1C11C1C1)C1 | LOGP:1C11C1C1C1 | LOGP:11 | LOGP:1C11C1C1C1 | LOGP:::1)C1 | LOGP:1)C1\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: c1ccc(-n2ncc3c2ncn2cnnc32)cc1 | LOGP: 1.77 | QED: 0.52 | TPSA: 60.90 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 49])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:07] SMILES Parse Error: syntax error while parsing: fCC(=O)N1CCN(C(=O)c2occc2-c2ccccc2)CC1C(=O)OC)C1C)C1C\n",
      "[14:54:07] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:07] fCC(=O)N1CCN(C(=O)c2occc2-c2ccccc2)CC1C(=\n",
      "[14:54:07] ^\n",
      "[14:54:07] SMILES Parse Error: Failed parsing SMILES 'fCC(=O)N1CCN(C(=O)c2occc2-c2ccccc2)CC1C(=O)OC)C1C)C1C' for input: 'fCC(=O)N1CCN(C(=O)c2occc2-c2ccccc2)CC1C(=O)OC)C1C)C1C'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc(-n2ncc3c2ncn2cnnc32)cc1 | LOGP: 1.77 | QED: 0.52 | TPSA: 60.90 => fCc1nnc2c3c(nn2n1)c1cnn2-c1ccccc1F)C(F)F2F)FF1F | TPSA:F)F1FF)N1ccccc1F)F)F)F)F)F)F)F1FF1F1F1F)N1F)F1FF1F1F1F)F1F | TPSA:F1F1F1F1F1F1F | TPSA:F)N1F1F)N1F1F1F1F1F1F1CCO4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4O4\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(NCc1ncon1)NCC1(c2ccccc2)CC1 | LOGP: 2.19 | QED: 0.88 | TPSA: 80.05 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:09] SMILES Parse Error: syntax error while parsing: fCc1nnc2c3c(nn2n1)c1cnn2-c1ccccc1F)C(F)F2F)FF1F\n",
      "[14:54:09] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:09] fCc1nnc2c3c(nn2n1)c1cnn2-c1ccccc1F)C(F)F2\n",
      "[14:54:09] ^\n",
      "[14:54:09] SMILES Parse Error: Failed parsing SMILES 'fCc1nnc2c3c(nn2n1)c1cnn2-c1ccccc1F)C(F)F2F)FF1F' for input: 'fCc1nnc2c3c(nn2n1)c1cnn2-c1ccccc1F)C(F)F2F)FF1F'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(NCc1ncon1)NCC1(c2ccccc2)CC1 | LOGP: 2.19 | QED: 0.88 | TPSA: 80.05 => fCc1nc(CNC(=O)NCC2(c3ccc(F)cc3)CC2)no1 | TPSA: 88.0)no1 | TPSA: 101.0)0)0)C1)C1C1C1C1C1C1C1C1COCC1O1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1CO1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(CSc1ncn(-c2ccccc2)n1)N1CCc2ccccc21 | LOGP: 2.95 | QED: 0.69 | TPSA: 51.02 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 57])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:10] SMILES Parse Error: syntax error while parsing: fCc1nc(CNC(=O)NCC2(c3ccc(F)cc3)CC2)no1\n",
      "[14:54:10] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:10] fCc1nc(CNC(=O)NCC2(c3ccc(F)cc3)CC2)no1\n",
      "[14:54:10] ^\n",
      "[14:54:10] SMILES Parse Error: Failed parsing SMILES 'fCc1nc(CNC(=O)NCC2(c3ccc(F)cc3)CC2)no1' for input: 'fCc1nc(CNC(=O)NCC2(c3ccc(F)cc3)CC2)no1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(CSc1ncn(-c2ccccc2)n1)N1CCc2ccccc21 | LOGP: 2.95 | QED: 0.69 | TPSA: 51.02 => fO=C(CSc1ncn(-c2ccccc2)n1)N1CCc2ccccc21F)N1CC1FFFF1FF | TFF)F1F1FF1FFF | TPSA:n1F)F)F | TPSA:F | TPSA:F1F)F)F1F1F1F1F1F | TPSA:1F1F1F1F | TPSA:1F)N1F1F1F1F1F1F11F1F)N11F1F1F1F1F1cncccccccccccccccccccccccccccccc1F1N1F)N1F)F1N1F1 | Tnn1F1F1F1 | TO4nn\n",
      "Validity: 0.00\n",
      "Uniqueness: 0.00\n",
      "Average Tanimoto similarity to training set: 0.00\n",
      "\n",
      "Quick evaluation with 100 samples...\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(NC1CCNCC1)c1ccc2[nH]cnc2c1 | LOGP: 1.52 | QED: 0.88 | TPSA: 87.32 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:11] SMILES Parse Error: syntax error while parsing: fO=C(CSc1ncn(-c2ccccc2)n1)N1CCc2ccccc21F)N1CC1FFFF1FF\n",
      "[14:54:11] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:11] fO=C(CSc1ncn(-c2ccccc2)n1)N1CCc2ccccc21F)\n",
      "[14:54:11] ^\n",
      "[14:54:11] SMILES Parse Error: Failed parsing SMILES 'fO=C(CSc1ncn(-c2ccccc2)n1)N1CCc2ccccc21F)N1CC1FFFF1FF' for input: 'fO=C(CSc1ncn(-c2ccccc2)n1)N1CCc2ccccc21F)N1CC1FFFF1FF'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(NC1CCNCC1)c1ccc2[nH]cnc2c1 | LOGP: 1.52 | QED: 0.88 | TPSA: 87.32 => fCCOC(=O)N1CCC(NC(=O)c2ccc3[nH]cnc3c2)CC1COCOC2C1C1)C1C1C1C1 | | TA:A:C1 | TPSA:2)C1)C1C1)n1)N1)C1C1)N1F)C1C1F)N1C1 | TPSA:2)C1C1C1F)C1F)N1C1C1C1C1C1C1C1C1C1F)C1C1C1C1C1C1C1C1C1C1C1)C1C1C1C1C1 | LOGP:F)C1 | LOGP:1C1C1C1C1C1C1 | LOGP:1\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: c1ccc2c(c1)CCNC2 | LOGP: 3.13 | QED: 0.76 | TPSA: 12.03 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 41])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:12] SMILES Parse Error: syntax error while parsing: fCCOC(=O)N1CCC(NC(=O)c2ccc3[nH]cnc3c2)CC1COCOC2C1C1)C1C1C1C1\n",
      "[14:54:12] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:12] fCCOC(=O)N1CCC(NC(=O)c2ccc3[nH]cnc3c2)CC1\n",
      "[14:54:12] ^\n",
      "[14:54:12] SMILES Parse Error: Failed parsing SMILES 'fCCOC(=O)N1CCC(NC(=O)c2ccc3[nH]cnc3c2)CC1COCOC2C1C1)C1C1C1C1' for input: 'fCCOC(=O)N1CCC(NC(=O)c2ccc3[nH]cnc3c2)CC1COCOC2C1C1)C1C1C1C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc2c(c1)CCNC2 | LOGP: 3.13 | QED: 0.76 | TPSA: 12.03 => fFc1ccc2c(c1Br)C(F)(F)NCC2COC(F)F)FF1F2F1F | TPSA:1F | TPSA:1F | TPSA:1F1F | QED:1F)N1F)F)N1CC1CCOCCO1F)F1F)N1F)F)N1F1F)N1CCO)F)N1CCOCCO1CCO1CCOCCO1F1F1F1F1F1F)N1F1F1F1CCOCCO1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(NCCN1CCCCCC1=O)c1cncs1 | LOGP: 1.89 | QED: 0.92 | TPSA: 62.30 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:13] SMILES Parse Error: syntax error while parsing: fFc1ccc2c(c1Br)C(F)(F)NCC2COC(F)F)FF1F2F1F\n",
      "[14:54:13] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:13] fFc1ccc2c(c1Br)C(F)(F)NCC2COC(F)F)FF1F2F1\n",
      "[14:54:13] ^\n",
      "[14:54:13] SMILES Parse Error: Failed parsing SMILES 'fFc1ccc2c(c1Br)C(F)(F)NCC2COC(F)F)FF1F2F1F' for input: 'fFc1ccc2c(c1Br)C(F)(F)NCC2COC(F)F)FF1F2F1F'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(NCCN1CCCCCC1=O)c1cncs1 | LOGP: 1.89 | QED: 0.92 | TPSA: 62.30 => fCc1ncsc1C(=O)NCCN1CCCCCC1=O | TPSA: 75.2 | TPSA: 82.3)CCCC1=O)c1cncs1 | TPSA:)N1CCCCCC1=O)N1 | TPSA: 62.3)C#N1C#N)C#N)C#N)C#N1C#N)C#C#C#N)C#N1C#N)C1C1C1C#N1CCO4 | TPSA:3 | TPSA:3)C#N1C1C#N1C#N1C#N)C2)C#N1C#N1C1C1C2)C2)C2)C2)C2)C#N2)C2)C2)C2)C2)C\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(CCNC(=O)C1CC1)NCC1CC=CCC1 | LOGP: 1.62 | QED: 0.72 | TPSA: 58.20 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:15] SMILES Parse Error: syntax error while parsing: fCc1ncsc1C(=O)NCCN1CCCCCC1=O\n",
      "[14:54:15] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:15] fCc1ncsc1C(=O)NCCN1CCCCCC1=O\n",
      "[14:54:15] ^\n",
      "[14:54:15] SMILES Parse Error: Failed parsing SMILES 'fCc1ncsc1C(=O)NCCN1CCCCCC1=O' for input: 'fCc1ncsc1C(=O)NCCN1CCCCCC1=O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(CCNC(=O)C1CC1)NCC1CC=CCC1 | LOGP: 1.62 | QED: 0.72 | TPSA: 58.20 => fO=C(CCNC(=O)C1CC1)NCC1CC=CCC1F)N(F)F)C1CC1 | TPSA:F)F)F1111111111111111111 | T1)F1)F1F1)F1F41F4CO4CO4CO4CO4CO4CO4CO4CO4CO4CO4CO4CO4CO4CO4)F31F4)CO4CO4CO4)CO4)CO4CO4)C2C2)CO4CO4CO4CO4C2)C2)F31F31F4)F31)C2)CO4CO2C2C2)F31F31)F4)CO4C2F31F31C2C2)C2)C2F31C2C2)\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(Cc1ccccc1)NCc1cn[nH]c1 | LOGP: 2.28 | QED: 0.84 | TPSA: 56.15 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:16] SMILES Parse Error: syntax error while parsing: fO=C(CCNC(=O)C1CC1)NCC1CC=CCC1F)N(F)F)C1CC1\n",
      "[14:54:16] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:16] fO=C(CCNC(=O)C1CC1)NCC1CC=CCC1F)N(F)F)C1C\n",
      "[14:54:16] ^\n",
      "[14:54:16] SMILES Parse Error: Failed parsing SMILES 'fO=C(CCNC(=O)C1CC1)NCC1CC=CCC1F)N(F)F)C1CC1' for input: 'fO=C(CCNC(=O)C1CC1)NCC1CC=CCC1F)N(F)F)C1CC1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Cc1ccccc1)NCc1cn[nH]c1 | LOGP: 2.28 | QED: 0.84 | TPSA: 56.15 => fC#CCOc1ccc(CC(=O)NCc2c(C)nn(C)c2C)cc1FOC)cc1FFA:1F | TPSA:2)cc1F)cc1F)F4111F4 | TPSA:FO4)O4O4O4 | TPSA:2)N1F)N1F)F)N1F)C1F)N1F)N1F)C1F)O4O4 | TPSA:1CCO4)N1F)N1F)N1CCO4)N1F)N1CCO4)C#N1CCO4)C1F)N1CCO4)N1CCO4)N1CCO4)N1F)N1F1CCO4)N1CCO4)C1CC\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(C1CCOCC1)N1CCOC2CCC1C2OCC1CC1 | LOGP: 1.60 | QED: 0.79 | TPSA: 48.00 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:17] SMILES Parse Error: syntax error while parsing: fC#CCOc1ccc(CC(=O)NCc2c(C)nn(C)c2C)cc1FOC)cc1FFA:1F\n",
      "[14:54:17] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:17] fC#CCOc1ccc(CC(=O)NCc2c(C)nn(C)c2C)cc1FOC\n",
      "[14:54:17] ^\n",
      "[14:54:17] SMILES Parse Error: Failed parsing SMILES 'fC#CCOc1ccc(CC(=O)NCc2c(C)nn(C)c2C)cc1FOC)cc1FFA:1F' for input: 'fC#CCOc1ccc(CC(=O)NCc2c(C)nn(C)c2C)cc1FOC)cc1FFA:1F'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(C1CCOCC1)N1CCOC2CCC1C2OCC1CC1 | LOGP: 1.60 | QED: 0.79 | TPSA: 48.00 => fO=C(C1CCOCC1)N1CCOC2CCC1C2OCC1CC1)O2CC1)C2)C1F1FFFFFF4F)F1FF7F4F4F4F4F4F4F4F4F4F)F4F4F4F4F31F31 | TPSA:4CC1F4F31F21 | TPSA:4F21 | LOGP:4CCC1F4F4F31)F4C1F31)F31)F4C4F4C4C4C4C4C4F4C4C4F4O4C4C4C1C4C1CO4C1N1F31O4C1N1O1N1F4C2CO4C4C4CO1O4C4C4\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(Nc1nc[nH]n1)c1ccccc1 | LOGP: 1.19 | QED: 0.87 | TPSA: 88.91 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 49])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:18] SMILES Parse Error: syntax error while parsing: fO=C(C1CCOCC1)N1CCOC2CCC1C2OCC1CC1)O2CC1)C2)C1F1FFFFFF4F)F1FF7F4F4F4F4F4F4F4F4F4F)F4F4F4F4F31F31\n",
      "[14:54:18] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:18] fO=C(C1CCOCC1)N1CCOC2CCC1C2OCC1CC1)O2CC1)\n",
      "[14:54:18] ^\n",
      "[14:54:18] SMILES Parse Error: Failed parsing SMILES 'fO=C(C1CCOCC1)N1CCOC2CCC1C2OCC1CC1)O2CC1)C2)C1F1FFFFFF4F)F1FF7F4F4F4F4F4F4F4F4F4F)F4F4F4F4F31F31' for input: 'fO=C(C1CCOCC1)N1CCOC2CCC1C2OCC1CC1)O2CC1)C2)C1F1FFFFFF4F)F1FF7F4F4F4F4F4F4F4F4F4F)F4F4F4F4F31F31'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Nc1nc[nH]n1)c1ccccc1 | LOGP: 1.19 | QED: 0.87 | TPSA: 88.91 => fCC(=O)Nc1ccc(C(=O)Nc2ncn(C)n2)cc1OC(F)F | TPSA:F)cc1F | TPSA: 99.1CC1)C1)C111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(CCc1ccccc1)NCC1CCCO1 | LOGP: 2.12 | QED: 0.90 | TPSA: 62.12 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 46])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:19] SMILES Parse Error: syntax error while parsing: fCC(=O)Nc1ccc(C(=O)Nc2ncn(C)n2)cc1OC(F)F\n",
      "[14:54:19] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:19] fCC(=O)Nc1ccc(C(=O)Nc2ncn(C)n2)cc1OC(F)F\n",
      "[14:54:19] ^\n",
      "[14:54:19] SMILES Parse Error: Failed parsing SMILES 'fCC(=O)Nc1ccc(C(=O)Nc2ncn(C)n2)cc1OC(F)F' for input: 'fCC(=O)Nc1ccc(C(=O)Nc2ncn(C)n2)cc1OC(F)F'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(CCc1ccccc1)NCC1CCCO1 | LOGP: 2.12 | QED: 0.90 | TPSA: 62.12 => fCOc1cccc(CCC(=O)NCC2CCCO2)c1OC(C)C2C1C1)C2 | TPSA:2 | TPSA:2)C1C1O1C1O1)C1C1)O1C1F1F)C1F)C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1C1C1C1C1)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1 | LOGPPPP:C1C1C1C1O1C1C1C1)C1O1C1C1C1C1C1C1O1O1C1C1C1C1)C1C\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(NC1CCNCC1)c1ccc[nH]1 | LOGP: 0.98 | QED: 0.82 | TPSA: 74.43 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:20] SMILES Parse Error: syntax error while parsing: fCOc1cccc(CCC(=O)NCC2CCCO2)c1OC(C)C2C1C1)C2\n",
      "[14:54:20] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:20] fCOc1cccc(CCC(=O)NCC2CCCO2)c1OC(C)C2C1C1)\n",
      "[14:54:20] ^\n",
      "[14:54:20] SMILES Parse Error: Failed parsing SMILES 'fCOc1cccc(CCC(=O)NCC2CCCO2)c1OC(C)C2C1C1)C2' for input: 'fCOc1cccc(CCC(=O)NCC2CCCO2)c1OC(C)C2C1C1)C2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(NC1CCNCC1)c1ccc[nH]1 | LOGP: 0.98 | QED: 0.82 | TPSA: 74.43 => fCOC(=O)C1CCN(C(=O)c2cc(C#N)cn2C)CC1C(C)C)CC1C)C111C)C111 | TC1)C1 | TPSA | TPSA:)N1)O)[n1)C1)N1)C1)O)=O)[n1)C1)C1)N1)N1)C1C1C1)C1C1C1)C1C1C1C1)C1)C1C1C1C1C1)C1C1C1C1C1C1CC1C1C1C1C1C1C1C1C1C1C1C#N1C1C1C1)C#N1C1O)=O)=O)=O1)N1C#N1)\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(NCc1nnc2n1CCCCC2)C1CCNCC1 | LOGP: 0.51 | QED: 0.84 | TPSA: 89.35 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:21] SMILES Parse Error: syntax error while parsing: fCOC(=O)C1CCN(C(=O)c2cc(C#N)cn2C)CC1C(C)C)CC1C)C111C)C111\n",
      "[14:54:21] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:21] fCOC(=O)C1CCN(C(=O)c2cc(C#N)cn2C)CC1C(C)C\n",
      "[14:54:21] ^\n",
      "[14:54:21] SMILES Parse Error: Failed parsing SMILES 'fCOC(=O)C1CCN(C(=O)c2cc(C#N)cn2C)CC1C(C)C)CC1C)C111C)C111' for input: 'fCOC(=O)C1CCN(C(=O)c2cc(C#N)cn2C)CC1C(C)C)CC1C)C111C)C111'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(NCc1nnc2n1CCCCC2)C1CCNCC1 | LOGP: 0.51 | QED: 0.84 | TPSA: 89.35 => fCOCC(=O)N1CCC(C(=O)NCc2nnc3n2CCCCC3)CC1C1CC1 | TPSA:)C1111111111111111111111111 | TPSA:)C1)C1)C1)C1)C1C1)C1)C111)C1)C111C11111111111111111111111111111111111111111111114CO2 | LOGP:C1C2CO2C2 | LOGP:C14CO2CO2CO2C2C2C2C2 | LOGP:F2CO2CO1C2CO2CO2C2 |\n",
      "Validity: 0.00\n",
      "Uniqueness: 0.00\n",
      "Average Tanimoto similarity to training set: 0.00\n",
      "\n",
      "Generating 5 example molecules...\n",
      "\n",
      "Example 1:\n",
      "Scaffold: c1ccc2c(c1)NCCO2\n",
      "Requested LogP: 0.06\n",
      "Requested QED: 0.81\n",
      "Requested TPSA: 70.67\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: c1ccc2c(c1)NCCO2 | LOGP: 0.06 | QED: 0.81 | TPSA: 70.67 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:23] SMILES Parse Error: syntax error while parsing: fCOCC(=O)N1CCC(C(=O)NCc2nnc3n2CCCCC3)CC1C1CC1\n",
      "[14:54:23] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:23] fCOCC(=O)N1CCC(C(=O)NCc2nnc3n2CCCCC3)CC1C\n",
      "[14:54:23] ^\n",
      "[14:54:23] SMILES Parse Error: Failed parsing SMILES 'fCOCC(=O)N1CCC(C(=O)NCc2nnc3n2CCCCC3)CC1C1CC1' for input: 'fCOCC(=O)N1CCC(C(=O)NCc2nnc3n2CCCCC3)CC1C1CC1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc2c(c1)NCCO2 | LOGP: 0.06 | QED: 0.81 | TPSA: 70.67 => fCNC(=O)CNC(=O)CN1CCOc2ccc(F)cc21 | TPSA: 87.7 => fOCC1Cc1ccccc1F)C1C(=O)NCCO1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)N1C1C1C1 | LOGP:1C1)O1)O1C1C11C1C1 | LOGP:: 3.77 | TPSA: 3.75 | TPSA::: 0.89 | TPSA:::: 0.75 | TPSA: 3.2)N1C1C1)N1)N1C1C1C1C1C1C1C1C1C1\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc2c(c1)NCCO2 | LOGP: 0.06 | QED: 0.81 | TPSA: 70.67 => fCC(=O)NCC(=O)N1CCOc2ccc(F)cc21 | TPSA: 111.9 => fOCCO)C1OCCOc1ccccc1N1CCO1)C(C)=O)O1)C111C1O1C1C1C#N1C1C1C1C1C#N1C#N1C1C#N1C1C1F)C1C1C1C1C1C1C1C1C1C1C1 | LOGP::1C1C1C1C1C1 | LOGP:: | LOGP::::::: 0.75 | TPSA:: TPSA:: TPSA: TPSA::: TPSA: TPSA: TPSA: TPSA: TPSA:O4C#N)C#\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc2c(c1)NCCO2 | LOGP: 0.06 | QED: 0.81 | TPSA: 70.67 => fCCNC(=O)CN1CCOc2ccc(C(=O)NCCO)cc21 | TPSA: 85.5 => fO)cc1F)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1)O1C1 | TPSA:::::: 0.4N1C1C1)C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1 | LOGP::::::::::::::::::::: 0.78 | TPSA::: 0.73 | TPSA: 3.4C1C#N)=N1(N)=O4N)=O4N)=O4N1C1C1\n",
      "  Generated SMILES 1: fOCC1Cc1ccccc1F)C1C(=O)NCCO1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)N1C1C1C1 | LOGP:1C1)O1)O1C1C11C1C1 | LOGP:: 3.77 | TPSA: 3.75 | TPSA::: 0.89 | TPSA:::: 0.75 | TPSA: 3.2)N1C1C1)N1)N1C1C1C1C1C1C1C1C1C1 (INVALID)\n",
      "  Generated SMILES 2: fOCCO)C1OCCOc1ccccc1N1CCO1)C(C)=O)O1)C111C1O1C1C1C#N1C1C1C1C1C#N1C#N1C1C#N1C1C1F)C1C1C1C1C1C1C1C1C1C1C1 | LOGP::1C1C1C1C1C1 | LOGP:: | LOGP::::::: 0.75 | TPSA:: TPSA:: TPSA: TPSA::: TPSA: TPSA: TPSA: TPSA: TPSA:O4C#N)C# (INVALID)\n",
      "  Generated SMILES 3: fO)cc1F)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1)O1C1 | TPSA:::::: 0.4N1C1C1)C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1 | LOGP::::::::::::::::::::: 0.78 | TPSA::: 0.73 | TPSA: 3.4C1C#N)=N1(N)=O4N)=O4N)=O4N1C1C1 (INVALID)\n",
      "\n",
      "Example 2:\n",
      "Scaffold: O=C(CNc1nnnn1-c1ccccc1)N1CCCC1\n",
      "Requested LogP: 0.70\n",
      "Requested QED: 0.89\n",
      "Requested TPSA: 75.94\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(CNc1nnnn1-c1ccccc1)N1CCCC1 | LOGP: 0.70 | QED: 0.89 | TPSA: 75.94 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 51])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:24] SMILES Parse Error: syntax error while parsing: fOCC1Cc1ccccc1F)C1C(=O)NCCO1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)N1C1C1C1\n",
      "[14:54:24] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:24] fOCC1Cc1ccccc1F)C1C(=O)NCCO1C1C)C1C1C1C1C\n",
      "[14:54:24] ^\n",
      "[14:54:24] SMILES Parse Error: Failed parsing SMILES 'fOCC1Cc1ccccc1F)C1C(=O)NCCO1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)N1C1C1C1' for input: 'fOCC1Cc1ccccc1F)C1C(=O)NCCO1C1C)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)N1C1C1C1'\n",
      "[14:54:24] SMILES Parse Error: syntax error while parsing: fOCCO)C1OCCOc1ccccc1N1CCO1)C(C)=O)O1)C111C1O1C1C1C#N1C1C1C1C1C#N1C#N1C1C#N1C1C1F)C1C1C1C1C1C1C1C1C1C1C1\n",
      "[14:54:24] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:24] fOCCO)C1OCCOc1ccccc1N1CCO1)C(C)=O)O1)C111\n",
      "[14:54:24] ^\n",
      "[14:54:24] SMILES Parse Error: Failed parsing SMILES 'fOCCO)C1OCCOc1ccccc1N1CCO1)C(C)=O)O1)C111C1O1C1C1C#N1C1C1C1C1C#N1C#N1C1C#N1C1C1F)C1C1C1C1C1C1C1C1C1C1C1' for input: 'fOCCO)C1OCCOc1ccccc1N1CCO1)C(C)=O)O1)C111C1O1C1C1C#N1C1C1C1C1C#N1C#N1C1C#N1C1C1F)C1C1C1C1C1C1C1C1C1C1C1'\n",
      "[14:54:24] SMILES Parse Error: syntax error while parsing: fO)cc1F)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1)O1C1\n",
      "[14:54:24] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:24] fO)cc1F)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C\n",
      "[14:54:24] ^\n",
      "[14:54:24] SMILES Parse Error: Failed parsing SMILES 'fO)cc1F)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1)O1C1' for input: 'fO)cc1F)C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1)O1C1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(CNc1nnnn1-c1ccccc1)N1CCCC1 | LOGP: 0.70 | QED: 0.89 | TPSA: 75.94 => fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFF)FFF1FF1F | TPSAA:44 | TPSA:74)nnnnnn1F)N1)F)N1F)F)F)N1F)F1F)N1F41F1F1F)N1F1F)F1F41F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F41F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(CNc1nnnn1-c1ccccc1)N1CCCC1 | LOGP: 0.70 | QED: 0.89 | TPSA: 75.94 => fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFFFFFF | Tcc1FF | T4F)FF1F44F111F1F | TPSA:F)F1F4 | TPSA:F4 | TPSA:F4 | TPSA:F)N1F9F1F9F4cncccccccccccccccccc1O4cn1F)N1F)N1F)no1F1F)on1F1F)F4cn1F1F1F1F1F1O41F1F41F1F1F1O41F411F11F11F1F1111F1F1F411O4111\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(CNc1nnnn1-c1ccccc1)N1CCCC1 | LOGP: 0.70 | QED: 0.89 | TPSA: 75.94 => fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFF)FFFF1F1F | TPSA:F | TPSA:74)F1F1F4)N1F)N1ccccccccccccccn1F)N1F)F)F)F)F1F)F)N1F1F1F1F1F1F1F1F1F)N1F)F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F 4.111F11F 4.41F1F1F1F1F 4.41F1nnnnnnnnnn1F1F1F1F1N1\n",
      "  Generated SMILES 1: fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFF)FFF1FF1F | TPSAA:44 | TPSA:74)nnnnnn1F)N1)F)N1F)F)F)N1F)F1F)N1F41F1F1F)N1F1F)F1F41F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F41F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F (INVALID)\n",
      "  Generated SMILES 2: fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFFFFFF | Tcc1FF | T4F)FF1F44F111F1F | TPSA:F)F1F4 | TPSA:F4 | TPSA:F4 | TPSA:F)N1F9F1F9F4cncccccccccccccccccc1O4cn1F)N1F)N1F)no1F1F)on1F1F)F4cn1F1F1F1F1F1O41F1F41F1F1F1O41F411F11F11F1F1111F1F1F411O4111 (INVALID)\n",
      "  Generated SMILES 3: fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFF)FFFF1F1F | TPSA:F | TPSA:74)F1F1F4)N1F)N1ccccccccccccccn1F)N1F)F)F)F)F1F)F)N1F1F1F1F1F1F1F1F1F)N1F)F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F1F 4.111F11F 4.41F1F1F1F1F 4.41F1nnnnnnnnnn1F1F1F1F1N1 (INVALID)\n",
      "\n",
      "Example 3:\n",
      "Scaffold: c1ccc(-c2cscn2)cc1\n",
      "Requested LogP: 3.89\n",
      "Requested QED: 0.89\n",
      "Requested TPSA: 71.09\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: c1ccc(-c2cscn2)cc1 | LOGP: 3.89 | QED: 0.89 | TPSA: 71.09 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:25] SMILES Parse Error: syntax error while parsing: fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFF)FFF1FF1F\n",
      "[14:54:25] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:25] fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1\n",
      "[14:54:25] ^\n",
      "[14:54:25] SMILES Parse Error: Failed parsing SMILES 'fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFF)FFF1FF1F' for input: 'fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFF)FFF1FF1F'\n",
      "[14:54:25] SMILES Parse Error: syntax error while parsing: fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFFFFFF\n",
      "[14:54:25] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:25] fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1\n",
      "[14:54:25] ^\n",
      "[14:54:25] SMILES Parse Error: Failed parsing SMILES 'fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFFFFFF' for input: 'fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFFFFFFFF'\n",
      "[14:54:25] SMILES Parse Error: syntax error while parsing: fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFF)FFFF1F1F\n",
      "[14:54:25] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:25] fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1\n",
      "[14:54:25] ^\n",
      "[14:54:25] SMILES Parse Error: Failed parsing SMILES 'fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFF)FFFF1F1F' for input: 'fO=C(CNc1nnnn1-c1ccccc1)N1CCCC1CO)N1CCCC1FFFF)FFFF1F1F'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc(-c2cscn2)cc1 | LOGP: 3.89 | QED: 0.89 | TPSA: 71.09 => fCC(C)(C)OC(=O)Nc1nc(-c2ccccc2)cs1 | TCCO)c1C(C)C1ccccc1F)O1111111O1F 4.1F1F1F4O1F1F4)F4F4)F1F)F1F4F4 | T1F4O4O4F4)N1F4)F4)N44 | TPS1F4O4)C1F444 | TPSA:9F4CO444CO4CO42F4F4F4CO4O4)N42F1F4)N4N1F4N44O42)N4O4F42F42F42F4F4N4F4)N1F2)N42F1F1F2F2F4\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc(-c2cscn2)cc1 | LOGP: 3.89 | QED: 0.89 | TPSA: 71.09 => fCC(C)C(=O)Nc1nc(-c2ccc(Cl)cc2)cs1 | TPSA: 42.4)cc1C(=O)NC1C)C1C11C1C1C11 | LOGP: 3.1 | T1)C1C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1C1C1C1)C1)C1C1)C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1C1C1C1C1)C1C1C1C1)C1C1C1C1C11C1)C1C1C11C11C1 | LOGPPPPPPPPPPPPPP:\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: c1ccc(-c2cscn2)cc1 | LOGP: 3.89 | QED: 0.89 | TPSA: 71.09 => fCC(=O)Nc1nc(-c2ccc(NC(=O)CC(C)C)cc2)cs1 | TPSA: 42.0)cs1 | TPSA:)C1CC1)C11111111111111111111)F1 | LOGP:::1)F111111)C11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "  Generated SMILES 1: fCC(C)(C)OC(=O)Nc1nc(-c2ccccc2)cs1 | TCCO)c1C(C)C1ccccc1F)O1111111O1F 4.1F1F1F4O1F1F4)F4F4)F1F)F1F4F4 | T1F4O4O4F4)N1F4)F4)N44 | TPS1F4O4)C1F444 | TPSA:9F4CO444CO4CO42F4F4F4CO4O4)N42F1F4)N4N1F4N44O42)N4O4F42F42F42F4F4N4F4)N1F2)N42F1F1F2F2F4 (INVALID)\n",
      "  Generated SMILES 2: fCC(C)C(=O)Nc1nc(-c2ccc(Cl)cc2)cs1 | TPSA: 42.4)cc1C(=O)NC1C)C1C11C1C1C11 | LOGP: 3.1 | T1)C1C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1C1C1C1)C1)C1C1)C1C1C1C1C1C1C1C1C1C1C1C1C1C1)C1C1C1C1C1C1C1)C1C1C1C1)C1C1C1C1C11C1)C1C1C11C11C1 | LOGPPPPPPPPPPPPPP: (INVALID)\n",
      "  Generated SMILES 3: fCC(=O)Nc1nc(-c2ccc(NC(=O)CC(C)C)cc2)cs1 | TPSA: 42.0)cs1 | TPSA:)C1CC1)C11111111111111111111)F1 | LOGP:::1)F111111)C11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 (INVALID)\n",
      "\n",
      "Example 4:\n",
      "Scaffold: O=C1CCN(CCC(=O)c2ccccc2)CC1\n",
      "Requested LogP: 2.55\n",
      "Requested QED: 0.78\n",
      "Requested TPSA: 46.61\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C1CCN(CCC(=O)c2ccccc2)CC1 | LOGP: 2.55 | QED: 0.78 | TPSA: 46.61 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 49])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:26] SMILES Parse Error: syntax error while parsing: fCC(C)(C)OC(=O)Nc1nc(-c2ccccc2)cs1\n",
      "[14:54:26] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:26] fCC(C)(C)OC(=O)Nc1nc(-c2ccccc2)cs1\n",
      "[14:54:26] ^\n",
      "[14:54:26] SMILES Parse Error: Failed parsing SMILES 'fCC(C)(C)OC(=O)Nc1nc(-c2ccccc2)cs1' for input: 'fCC(C)(C)OC(=O)Nc1nc(-c2ccccc2)cs1'\n",
      "[14:54:26] SMILES Parse Error: syntax error while parsing: fCC(C)C(=O)Nc1nc(-c2ccc(Cl)cc2)cs1\n",
      "[14:54:26] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:26] fCC(C)C(=O)Nc1nc(-c2ccc(Cl)cc2)cs1\n",
      "[14:54:26] ^\n",
      "[14:54:26] SMILES Parse Error: Failed parsing SMILES 'fCC(C)C(=O)Nc1nc(-c2ccc(Cl)cc2)cs1' for input: 'fCC(C)C(=O)Nc1nc(-c2ccc(Cl)cc2)cs1'\n",
      "[14:54:26] SMILES Parse Error: syntax error while parsing: fCC(=O)Nc1nc(-c2ccc(NC(=O)CC(C)C)cc2)cs1\n",
      "[14:54:26] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:26] fCC(=O)Nc1nc(-c2ccc(NC(=O)CC(C)C)cc2)cs1\n",
      "[14:54:26] ^\n",
      "[14:54:26] SMILES Parse Error: Failed parsing SMILES 'fCC(=O)Nc1nc(-c2ccc(NC(=O)CC(C)C)cc2)cs1' for input: 'fCC(=O)Nc1nc(-c2ccc(NC(=O)CC(C)C)cc2)cs1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C1CCN(CCC(=O)c2ccccc2)CC1 | LOGP: 2.55 | QED: 0.78 | TPSA: 46.61 => fO=C1CCN(CCC(=O)c2ccc(Cl)cc2)CC1=O | TPSA: 75.4)CC1C1C1C1C1C1C1C1C1C1C1C1 | LOGP:C1C1 | TPSA: 2.6 | TPSA:2 | TPSA:2=O4)C1C1C1)C1)C1C1)C1C1C1C1C1)C1C1C1C1C1C1)C1C1)C1C1C1)C1(N)=O)=O4C1C1(C#N)=O4C#N)=O)=O4N)=O4C#N)=O4C#N)=O4O4O4O4CO4O1O4O4O4O4O1\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C1CCN(CCC(=O)c2ccccc2)CC1 | LOGP: 2.55 | QED: 0.78 | TPSA: 46.61 => fC=CCOc1cccc(C(=O)CCN2CCC(=O)CC2)c1F)C1CC1C)C1C)C1C1C1 | TPSA: | TPSA:2)C1C1)C1C1C1C1C1C1)C1C1)C1C1)C1C1C1C1C1C1C1)C1C1C1C1C1)C1)C1C1C1C1C1C11C1)C11C1C1C1)C1C11C1C1C1)C1 | LOGP: | LOGP: | LOGP::::1C1C1C1C1 | LOGP::1C1C1C1C1C1C1C1C1)C1)C1\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C1CCN(CCC(=O)c2ccccc2)CC1 | LOGP: 2.55 | QED: 0.78 | TPSA: 46.61 => fC=CCOc1ccc(C(=O)CCN2CCC(=O)CC2)cc1F)cc1F | TPSA:FFOLD:FFOLD:)C1CC1 | TPSA:)C1CC1 | T1 | T1)cc1)F)nn1C1C1)N)C1C1C1)C1CC1)C1C1)C1C1C1C1C1C1C1C1)N1C1C1C1C1)C1C1C1C1 | TPSA:1C1C1O1C1C1C1C1C1C1C1C1C1C1C1)C1 | LOGP:1 | LOGP:1C1 | LOGP:1C1C1C1C1O1 | LOGP:1C1C1C\n",
      "  Generated SMILES 1: fO=C1CCN(CCC(=O)c2ccc(Cl)cc2)CC1=O | TPSA: 75.4)CC1C1C1C1C1C1C1C1C1C1C1C1 | LOGP:C1C1 | TPSA: 2.6 | TPSA:2 | TPSA:2=O4)C1C1C1)C1)C1C1)C1C1C1C1C1)C1C1C1C1C1C1)C1C1)C1C1C1)C1(N)=O)=O4C1C1(C#N)=O4C#N)=O)=O4N)=O4C#N)=O4C#N)=O4O4O4O4CO4O1O4O4O4O4O1 (INVALID)\n",
      "  Generated SMILES 2: fC=CCOc1cccc(C(=O)CCN2CCC(=O)CC2)c1F)C1CC1C)C1C)C1C1C1 | TPSA: | TPSA:2)C1C1)C1C1C1C1C1C1)C1C1)C1C1)C1C1C1C1C1C1C1)C1C1C1C1C1)C1)C1C1C1C1C1C11C1)C11C1C1C1)C1C11C1C1C1)C1 | LOGP: | LOGP: | LOGP::::1C1C1C1C1 | LOGP::1C1C1C1C1C1C1C1C1)C1)C1 (INVALID)\n",
      "  Generated SMILES 3: fC=CCOc1ccc(C(=O)CCN2CCC(=O)CC2)cc1F)cc1F | TPSA:FFOLD:FFOLD:)C1CC1 | TPSA:)C1CC1 | T1 | T1)cc1)F)nn1C1C1)N)C1C1C1)C1CC1)C1C1)C1C1C1C1C1C1C1C1)N1C1C1C1C1)C1C1C1C1 | TPSA:1C1C1O1C1C1C1C1C1C1C1C1C1C1C1)C1 | LOGP:1 | LOGP:1C1 | LOGP:1C1C1C1C1O1 | LOGP:1C1C1C (INVALID)\n",
      "\n",
      "Example 5:\n",
      "Scaffold: O=C(Oc1ccccc1)C1CC(=O)N(Cc2ccco2)C1\n",
      "Requested LogP: 2.80\n",
      "Requested QED: 0.63\n",
      "Requested TPSA: 59.75\n",
      "ç”Ÿæˆæ¡ä»¶: SCAFFOLD: O=C(Oc1ccccc1)C1CC(=O)N(Cc2ccco2)C1 | LOGP: 2.80 | QED: 0.63 | TPSA: 59.75 =>\n",
      "è¾“å…¥åºåˆ—é•¿åº¦: torch.Size([1, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:28] SMILES Parse Error: syntax error while parsing: fO=C1CCN(CCC(=O)c2ccc(Cl)cc2)CC1=O\n",
      "[14:54:28] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:28] fO=C1CCN(CCC(=O)c2ccc(Cl)cc2)CC1=O\n",
      "[14:54:28] ^\n",
      "[14:54:28] SMILES Parse Error: Failed parsing SMILES 'fO=C1CCN(CCC(=O)c2ccc(Cl)cc2)CC1=O' for input: 'fO=C1CCN(CCC(=O)c2ccc(Cl)cc2)CC1=O'\n",
      "[14:54:28] SMILES Parse Error: syntax error while parsing: fC=CCOc1cccc(C(=O)CCN2CCC(=O)CC2)c1F)C1CC1C)C1C)C1C1C1\n",
      "[14:54:28] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:28] fC=CCOc1cccc(C(=O)CCN2CCC(=O)CC2)c1F)C1CC\n",
      "[14:54:28] ^\n",
      "[14:54:28] SMILES Parse Error: Failed parsing SMILES 'fC=CCOc1cccc(C(=O)CCN2CCC(=O)CC2)c1F)C1CC1C)C1C)C1C1C1' for input: 'fC=CCOc1cccc(C(=O)CCN2CCC(=O)CC2)c1F)C1CC1C)C1C)C1C1C1'\n",
      "[14:54:28] SMILES Parse Error: syntax error while parsing: fC=CCOc1ccc(C(=O)CCN2CCC(=O)CC2)cc1F)cc1F\n",
      "[14:54:28] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:28] fC=CCOc1ccc(C(=O)CCN2CCC(=O)CC2)cc1F)cc1F\n",
      "[14:54:28] ^\n",
      "[14:54:28] SMILES Parse Error: Failed parsing SMILES 'fC=CCOc1ccc(C(=O)CCN2CCC(=O)CC2)cc1F)cc1F' for input: 'fC=CCOc1ccc(C(=O)CCN2CCC(=O)CC2)cc1F)cc1F'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Oc1ccccc1)C1CC(=O)N(Cc2ccco2)C1 | LOGP: 2.80 | QED: 0.63 | TPSA: 59.75 => fCc1ccc(C)c(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c1C(=O)N(C)C)C1C2C1C1C1C1C1C1C111C11C1C1C11C11C1C11C1C1C1C1C11C1C1C1C1C1C1111C1C11C11111C111C1111 | LOGP: 0. | LOGP: 0.96 | TPSA: 0.76 | TPSA: 0. | TPSA: 0.62 | TPSA:: 3. | TPSA:2 | TPSA:1C1C11111C1\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Oc1ccccc1)C1CC(=O)N(Cc2ccco2)C1 | LOGP: 2.80 | QED: 0.63 | TPSA: 59.75 => fCc1ccc(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c(C)c1C)C1C1CC1 | TPSA:)O1)O1)C1)C11)C11)C1111F)C1C1C1F)O1C111F)N1F)C1F)C1F)N1F)C1F)C1C1F)C1F)C1F)C1F)C1F1C1C1C1F)C1FF1C1FF4O4)C1C1C1C1CC1F)C1C1F1C1C1F4O4O4O4O1F)N1C1C1C1F)\n",
      "åŽŸå§‹ç”Ÿæˆæ–‡æœ¬: SCAFFOLD: O=C(Oc1ccccc1)C1CC(=O)N(Cc2ccco2)C1 | LOGP: 2.80 | QED: 0.63 | TPSA: 59.75 => fO=C(Oc1cccc(Cl)c1)C1CC(=O)N(Cc2ccco2)C1C1CC1 | TPSA: TPSA: 75.66111 | TPSA: 75.71)C1)C11)OCC1)N1)N11111F)N111F)C111111F)C1F)C1C111111111111F)N1C1C1F)C11C11C11C1C1C1C1CO1C1CO1CO1C1C1C1CO1CO1CO1CO1C1C1CO1CO1CO1CO1CO1CO1CO1CO1CO1CO1C1CO1CO\n",
      "  Generated SMILES 1: fCc1ccc(C)c(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c1C(=O)N(C)C)C1C2C1C1C1C1C1C1C111C11C1C1C11C11C1C11C1C1C1C1C11C1C1C1C1C1C1111C1C11C11111C111C1111 | LOGP: 0. | LOGP: 0.96 | TPSA: 0.76 | TPSA: 0. | TPSA: 0.62 | TPSA:: 3. | TPSA:2 | TPSA:1C1C11111C1 (INVALID)\n",
      "  Generated SMILES 2: fCc1ccc(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c(C)c1C)C1C1CC1 | TPSA:)O1)O1)C1)C11)C11)C1111F)C1C1C1F)O1C111F)N1F)C1F)C1F)N1F)C1F)C1C1F)C1F)C1F)C1F)C1F1C1C1C1F)C1FF1C1FF4O4)C1C1C1C1CC1F)C1C1F1C1C1F4O4O4O4O1F)N1C1C1C1F) (INVALID)\n",
      "  Generated SMILES 3: fO=C(Oc1cccc(Cl)c1)C1CC(=O)N(Cc2ccco2)C1C1CC1 | TPSA: TPSA: 75.66111 | TPSA: 75.71)C1)C11)OCC1)N1)N11111F)N111F)C111111F)C1F)C1C111111111111F)N1C1C1F)C11C11C11C1C1C1C1CO1C1CO1CO1C1C1C1CO1CO1CO1CO1C1C1CO1CO1CO1CO1CO1CO1CO1CO1CO1CO1C1CO1CO (INVALID)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:29] SMILES Parse Error: syntax error while parsing: fCc1ccc(C)c(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c1C(=O)N(C)C)C1C2C1C1C1C1C1C1C111C11C1C1C11C11C1C11C1C1C1C1C11C1C1C1C1C1C1111C1C11C11111C111C1111\n",
      "[14:54:29] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:29] fCc1ccc(C)c(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c\n",
      "[14:54:29] ^\n",
      "[14:54:29] SMILES Parse Error: Failed parsing SMILES 'fCc1ccc(C)c(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c1C(=O)N(C)C)C1C2C1C1C1C1C1C1C111C11C1C1C11C11C1C11C1C1C1C1C11C1C1C1C1C1C1111C1C11C11111C111C1111' for input: 'fCc1ccc(C)c(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c1C(=O)N(C)C)C1C2C1C1C1C1C1C1C111C11C1C1C11C11C1C11C1C1C1C1C11C1C1C1C1C1C1111C1C11C11111C111C1111'\n",
      "[14:54:29] SMILES Parse Error: syntax error while parsing: fCc1ccc(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c(C)c1C)C1C1CC1\n",
      "[14:54:29] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:29] fCc1ccc(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c(C)c\n",
      "[14:54:29] ^\n",
      "[14:54:29] SMILES Parse Error: Failed parsing SMILES 'fCc1ccc(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c(C)c1C)C1C1CC1' for input: 'fCc1ccc(OC(=O)C2CC(=O)N(Cc3ccco3)C2)c(C)c1C)C1C1CC1'\n",
      "[14:54:29] SMILES Parse Error: syntax error while parsing: fO=C(Oc1cccc(Cl)c1)C1CC(=O)N(Cc2ccco2)C1C1CC1\n",
      "[14:54:29] SMILES Parse Error: check for mistakes around position 1:\n",
      "[14:54:29] fO=C(Oc1cccc(Cl)c1)C1CC(=O)N(Cc2ccco2)C1C\n",
      "[14:54:29] ^\n",
      "[14:54:29] SMILES Parse Error: Failed parsing SMILES 'fO=C(Oc1cccc(Cl)c1)C1CC(=O)N(Cc2ccco2)C1C1CC1' for input: 'fO=C(Oc1cccc(Cl)c1)C1CC(=O)N(Cc2ccco2)C1C1CC1'\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, QED, rdMolDescriptors\n",
    "import torch\n",
    "\n",
    "# Assuming you have already loaded or have access to:\n",
    "# 1. The trained model (model)\n",
    "# 2. The tokenizer (tokenizer)\n",
    "# 3. The test dataset (test_df)\n",
    "# 4. The training dataset (train_df)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Run the evaluation with 1000 samples\n",
    "print(\"Starting model evaluation with 1000 samples...\")\n",
    "evaluate_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    reference_df=test_df,\n",
    "    n_samples=10\n",
    ")\n",
    "\n",
    "# If you want to run with fewer samples for a quicker test\n",
    "print(\"\\nQuick evaluation with 100 samples...\")\n",
    "evaluate_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    reference_df=test_df,\n",
    "    n_samples=10\n",
    ")\n",
    "\n",
    "# Optional: Save some example molecules as images\n",
    "def visualize_examples(model, tokenizer, test_df, num_examples=5):\n",
    "    print(f\"\\nGenerating {num_examples} example molecules...\")\n",
    "    example_rows = test_df.sample(n=num_examples)\n",
    "    \n",
    "    for i, row in enumerate(example_rows.itertuples()):\n",
    "        scaffold = row.Scaffold\n",
    "        logp_req = row.LogP\n",
    "        qed_req = row.QED\n",
    "        tpsa_req = row.TPSA\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Scaffold: {scaffold}\")\n",
    "        print(f\"Requested LogP: {logp_req:.2f}\")\n",
    "        print(f\"Requested QED: {qed_req:.2f}\")\n",
    "        print(f\"Requested TPSA: {tpsa_req:.2f}\")\n",
    "        \n",
    "        gen_smiles_list = generate_smiles_from_conditions(\n",
    "            model, tokenizer, scaffold, logp_req, qed_req, tpsa_req,\n",
    "            num_return_sequences=3\n",
    "        )\n",
    "        \n",
    "        for j, smi in enumerate(gen_smiles_list):\n",
    "            if is_valid_smiles(smi):\n",
    "                mol = Chem.MolFromSmiles(smi)\n",
    "                print(f\"  Generated SMILES {j+1}: {smi}\")\n",
    "                \n",
    "                # Calculate actual properties\n",
    "                gen_logp = Descriptors.MolLogP(mol)\n",
    "                gen_qed = QED.qed(mol)\n",
    "                gen_tpsa = Descriptors.TPSA(mol)\n",
    "                \n",
    "                print(f\"  Actual LogP: {gen_logp:.2f} (diff: {gen_logp - logp_req:.2f})\")\n",
    "                print(f\"  Actual QED: {gen_qed:.2f} (diff: {gen_qed - qed_req:.2f})\")\n",
    "                print(f\"  Actual TPSA: {gen_tpsa:.2f} (diff: {gen_tpsa - tpsa_req:.2f})\")\n",
    "            else:\n",
    "                print(f\"  Generated SMILES {j+1}: {smi} (INVALID)\")\n",
    "\n",
    "# Run the visualization function\n",
    "visualize_examples(model, tokenizer, test_df, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2d505-4786-4297-8fa1-b3d5d0b5d866",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7216726,
     "sourceId": 11509373,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7218172,
     "sourceId": 11511202,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
